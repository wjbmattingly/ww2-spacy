{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf7ba20-55df-4321-9800-fb7ffd168599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import glob\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "import toml\n",
    "\n",
    "from spacy import displacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e762319-5cbe-4476-8102-c659a9186990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mwt(text, label, tokenizer):\n",
    "    tokens = [t.text for t in tokenizer(text)]\n",
    "    pattern = []\n",
    "    for t in tokens:\n",
    "        if t == \"the\":\n",
    "            pattern.append({\"LOWER\": t})\n",
    "        else:\n",
    "            pattern.append({\"TEXT\": t})\n",
    "    pattern = {\"pattern\": pattern, \"label\": label}\n",
    "    return pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4b08eb2-8bf5-4b8a-ba40-85508f40854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(file):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read().splitlines()[1:]\n",
    "    file = file.replace(\"\\\\\", \"/\")\n",
    "    label = file.split(\"/\")[-1].upper().replace(\".TXT\", \"\")\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91ba08c0-cad1-41ef-bb7f-4c0cd9b03476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patterns(file, tokenizer, label=\"\"):\n",
    "    if label == \"\":\n",
    "        data, label = open_file(file)\n",
    "    else:\n",
    "        data, _ = open_file(file)\n",
    "    patterns = []\n",
    "    for d in data:\n",
    "        patterns.append(create_mwt(d, label, tokenizer))\n",
    "        if \"(\" in d:\n",
    "            patterns.append(create_mwt(d.split(\"(\")[0].strip(), label, tokenizer))\n",
    "            patterns.append(create_mwt(d.split(\"(\")[1].replace(\")\", \"\").strip(), label, tokenizer))\n",
    "        if \"USS\" in d or \"HMS\" in d:\n",
    "            patterns.append(create_mwt(d.split(\"(\")[0].strip().replace(\"USS\", \"the\").replace(\"HMS\", \"the\"), label, tokenizer))\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f2f5b71-d1a4-4341-b366-5848c3f9393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_military_patterns(file, tokenizer):\n",
    "    data, label = open_file(file)\n",
    "    patterns = []\n",
    "    for d in data:\n",
    "        patterns.append(create_mwt(d, label, tokenizer))\n",
    "        if d[0].isdigit():\n",
    "            patterns.append(create_mwt(\"the \"+d.split()[0].strip(), label, tokenizer))\n",
    "        if \"(\" in d:\n",
    "            patterns.append(create_mwt(d.split(\"(\")[0].strip(), label, tokenizer))\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f38ce5-3d4e-4d22-82ab-60d782877d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patterns_dir(directory, mode=\"\", label=\"\"):\n",
    "    tokenizer = spacy.blank(\"en\")\n",
    "    files = glob.glob(directory)\n",
    "    patterns = []\n",
    "    for file in files:\n",
    "        if mode == \"\":\n",
    "            patterns = patterns+create_patterns(file, tokenizer, label)\n",
    "        elif mode == \"military\":\n",
    "            patterns = patterns+create_military_patterns(file, tokenizer)\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fee2db0e-4b19-4440-b26b-9a38630e12b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_patterns(file, extra=\"\"):\n",
    "    patterns = \"\"\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read().splitlines()[1:]\n",
    "    for d in data:\n",
    "        patterns=patterns+f\"({d})|\"\n",
    "    patterns = f\"({patterns[:-1]}){extra}\"\n",
    "    print(patterns)\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c894aa6a-f039-4502-87f3-f6333a0f41e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"clean_spans\")\n",
    "def clean_spans(doc):\n",
    "    original_spans = list(doc.spans[\"ruler\"])\n",
    "    #remove the from spans\n",
    "    new_spans = []\n",
    "    for span in doc.spans[\"ruler\"]:\n",
    "        if span[0].text.lower() == \"the\":\n",
    "            span.start = span.start+1\n",
    "        new_spans.append(span)\n",
    "    doc.spans[\"ruler\"] = new_spans\n",
    "    \n",
    "    #filter the overlapping spans so that priority is given to the longest span unless all spans\n",
    "    #are of equal length\n",
    "    span_starts = [span.start for span in doc.spans[\"ruler\"]]\n",
    "    overlap_starts = set([i for i in span_starts if span_starts.count(i)>1])\n",
    "    longest = {}\n",
    "    for span in doc.spans[\"ruler\"]:\n",
    "        if span.start in overlap_starts:\n",
    "            if span.text not in longest:\n",
    "                longest[span.start] = [span.end, span.text]\n",
    "            else:\n",
    "                if longest[span.start][0] < span.end:\n",
    "                    longest[span.start] = [span.end, span.text]\n",
    "    final_spans = []\n",
    "    for span in doc.spans[\"ruler\"]:\n",
    "        if span.start in longest:\n",
    "            if [span.end, span.text] == longest[span.start]:\n",
    "                final_spans.append(span)\n",
    "        else:\n",
    "            final_spans.append(span)\n",
    "            \n",
    "    doc.spans[\"ruler\"] = final_spans\n",
    "    return doc\n",
    "\n",
    "@Language.component(\"military_personnel\")\n",
    "def military_personel(doc):\n",
    "    military_pattern = regex_patterns(\"assets/military_ranks/american/army.txt\",\n",
    "                                              extra=\" [A-Z][a-z\\.]*( [A-Z][a-z\\.]*)*\")\n",
    "    text = doc.text\n",
    "    new_ents = []\n",
    "    original_ents = list(doc.spans[\"ruler\"])\n",
    "    for sent in doc.sents:\n",
    "        for match in re.finditer(military_pattern, sent.text):\n",
    "            \n",
    "            start, end = match.span()\n",
    "            start = start+sent.start_char\n",
    "            end = end+sent.start_char\n",
    "            span = doc.char_span(start, end)\n",
    "            if span.text[-1] in string.punctuation:\n",
    "                span.end = span.end-1\n",
    "            start, end, name = span.start, span.end, span.text\n",
    "            original_ents.append(Span(doc, start, end, label=\"MILITARY_PERSONNEL\"))\n",
    "    doc.spans[\"ruler\"] = original_ents\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83ec46cb-b0b9-4e73-908d-d691d001c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"clean_tank\")\n",
    "def clean_tank(doc):\n",
    "    new_spans = []\n",
    "    for span in doc.spans[\"ruler\"]:\n",
    "        if span.label_ == \"TANK\":\n",
    "            if span.text.split()[-1] in [\"tank\", \"tanks\"]:\n",
    "                span.end = span.end-1\n",
    "        new_spans.append(span)\n",
    "    doc.spans[\"ruler\"] = new_spans\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12320d36-b640-44c7-a0f4-bf36aee644f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'toml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pipeline_data \u001b[38;5;241m=\u001b[39m \u001b[43mtoml\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./project.toml\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipeline_data\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      2\u001b[0m ships \u001b[38;5;241m=\u001b[39m patterns_dir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massets/ships/american/*.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m military_units \u001b[38;5;241m=\u001b[39m patterns_dir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massets/military_units/american/*.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmilitary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'toml' is not defined"
     ]
    }
   ],
   "source": [
    "pipeline_data = toml.load(\"./project.toml\")[\"pipeline_data\"]\n",
    "ships = patterns_dir(\"assets/ships/american/*.txt\")\n",
    "military_units = patterns_dir(\"assets/military_units/american/*.txt\", mode=\"military\")\n",
    "tanks = patterns_dir(\"assets/tanks/american/*.txt\")\n",
    "planes = patterns_dir(\"assets/planes/american/*.txt\")\n",
    "planes = patterns_dir(\"assets/weapons/american/*.txt\")\n",
    "battles = patterns_dir(\"assets/battles/*.txt\", label=\"BATTLE\")\n",
    "all_patterns = ships+military_units+tanks+planes+battles+weapons\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "ship_ruler = nlp.add_pipe(\"span_ruler\")\n",
    "ship_ruler.add_patterns(all_patterns)\n",
    "nlp.add_pipe(\"military_personnel\")\n",
    "nlp.add_pipe(\"clean_spans\")\n",
    "nlp.add_pipe(\"clean_tank\")\n",
    "for name, val in pipeline_data.items():\n",
    "    nlp.meta[name] = val\n",
    "nlp.to_disk(\"./models/ww2spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7ead9-789b-477b-a2e0-ccb3e126413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The P-35 flew in WW2 at Battle of Point 175. The 10th Armored Division was led by General William H. H. Morris. It contained 24 Sherman tanks. John Sherman is a false positive.\"\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style=\"span\", jupyter=True, options = {\"spans_key\": \"ruler\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de541874-47eb-4804-9f76-27b67463d7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ecf749-fd17-4d5c-ac91-4a9f93e3f727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
